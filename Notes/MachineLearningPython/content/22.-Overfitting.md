
## Overfitting

El overfitting es el sobre ajuste de los datos o dicho de otro modo la memorización de los datos por parte de la red neuronal

Para solucionarlo, se debe reducir la complejidad del modelo.

### Reducir overfitting con regularización

Con la regularización se penaliza a la función de coste, usando el valor de los pesos.

``` python
from keras import regularizer

model = models.Sequential()
model.add(
    layers.Dense(
        <int>,
        activation='<activation>',
        input_shape=(<shape_int>,),
        kernel_regularizer=regularizers.l1(0.0001)
    )
)

```

### Regularización L1 y L2:

#### Regularización L1 (Lasso)

Aplica una penalización proporcional al valor absoluto de los coeficientes de los pesos del modelo. Esto puede llevar a que algunos coeficientes se vuelvan exactamente cero, lo que implica una selección automática de características. La regularización L1 promueve la esparcidad en los pesos y puede ser útil cuando se desea realizar selección de características y reducir la complejidad del modelo.

``` python
    layers.Dense(
        <int>,
        activation='<activation>',
        input_shape=(<shape_int>,),
        kernel_regularizer=regularizers.l1(0.0001)
    )
```

#### Regularización L2 (Ridge)

Aplica una penalización proporcional al cuadrado de los coeficientes de los pesos del modelo. La regularización L2 penaliza los valores grandes de los pesos y empuja hacia valores más pequeños, lo que ayuda a evitar el sobreajuste. También se conoce como "regularización de peso al cuadrado" y es la más comúnmente utilizada en redes neuronales.

``` python
    layers.Dense(
        <int>,
        activation='<activation>',
        input_shape=(<shape_int>,),
        kernel_regularizer=regularizers.l2(0.0001)
    )
```

### Reducir el overfitting con dropout

El regularizador de Dropout es una técnica eficaz para reducir el sobreajuste en las redes neuronales. Durante el entrenamiento, se aplica de manera aleatoria y temporal a una fracción de las unidades (neuronas) de una capa, lo que implica que estas unidades no contribuyan a la propagación hacia adelante ni a la propagación hacia atrás durante ese paso. Al aplicar el abandono de forma estocástica, el modelo se ve obligado a aprender características redundantes y, en consecuencia, se vuelve más generalizable.

A nivel código, el dropout se agrega como si fuera una capa.

``` python
model.add(layers.Dropout(<flotante entre 0 y 1>))
```

### Regularización de elastic net

La regularización de elastic net es una combinación de la regularización L1 y L2. Aplica una penalización que incluye tanto términos de regularización L1 como L2, lo que combina las propiedades de ambas. La regularización de elastic net permite seleccionar características y al mismo tiempo manejar la multicolinealidad en los datos.