## Descenso del gradiente

El objetivo es disminuir la función de coste respecto a los pesos (weights) de cada neurona. Un punto bajo indica mayor precisión en las predicciones.

* Optimizador: me dice como debo actualizar los pesos, para disminuir el error. Hay varios tipos: gradient descent, AdaGrand, AdaDelta, Momentum, Nesterov, ADAM, RMS prop
* Learning rate: magnitud en la actualizacion de los pesos. Muy altos no encuentran un punto mínimo local o global, muy pequeños consumen demasiados recursos.
* Momentum: para evitar caer en mínimos globales hay optimizadores que aplican momentum (como el concepto de física, es el impulso extra que acumulo en la bajada de bajada) como el RMS prop, el cual es una variación del descenso del gradiente más el momentum.
