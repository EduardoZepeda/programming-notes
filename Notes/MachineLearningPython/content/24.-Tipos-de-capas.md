## Tipos de capas en Keras



### Capa Dense (totalmente conectada):

En una capa densa, cada neurona está conectada a todas las neuronas de la capa anterior. Cada conexión tiene un peso asociado que se aprende durante el entrenamiento. Las capas densas son capaces de aprender representaciones complejas y capturar patrones no lineales en los datos.

### Capa Conv2D (convolucional en 2D):

Las capas convolucionales aplican operaciones de convolución a los datos de entrada. Estas capas son especialmente útiles en tareas de visión por computadora, ya que pueden aprender patrones locales y características espaciales en imágenes o datos en formato de matriz.

### Capa LSTM (memoria a largo plazo):

Las capas LSTM son una variante de las capas recurrentes y están diseñadas para capturar relaciones secuenciales en los datos. A diferencia de las capas recurrentes estándar, las capas LSTM tienen una estructura más compleja que les permite aprender dependencias a largo plazo en las secuencias de datos.

Las capas LSTM se utilizan en problemas de procesamiento de lenguaje natural (NLP) y otras tareas que involucran secuencias de datos, como reconocimiento de voz y traducción automática.

### Capa recurrente (SimpleRNN, LSTM, GRU):

Las capas recurrentes son capaces de procesar secuencias y datos con dependencias temporales. Cada unidad en estas capas tiene una conexión recurrente consigo misma, lo que les permite tener memoria de estados anteriores.

Las capas recurrentes se utilizan en problemas de procesamiento del lenguaje natural (NLP) como la generación de texto, traducción automática y reconocimiento de voz, donde se necesita modelar la estructura temporal de los datos.

